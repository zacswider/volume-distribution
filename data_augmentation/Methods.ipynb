{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_layers(viewer):\n",
    "    layers = viewer.layers\n",
    "    while len(layers) > 0:\n",
    "        layers.remove(layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the cellpose segmentation file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the docs (https://cellpose.readthedocs.io/_/downloads/en/latest/pdf/) segmentations is a dictionary object with the following fields:\n",
    " - filename : filename of image\n",
    " - img : image with chosen channels (nchan x Ly x Lx) (if not multiplane)\n",
    " - masks : each pixel in the image is assigned to an ROI (0 = NO ROI; 1,2,. . . = ROI labels)\n",
    " - colors : colors for ROIs\n",
    " - outlines : outlines of ROIs (0 = NO outline; 1,2,. . . = outline labels)\n",
    " - chan_choose : channels that you chose in GUI (0=gray/none, 1=red, 2=green, 3=blue)\n",
    " - ismanual : element k = whether or not mask k was manually drawn or computed by the cellpose algorithm\n",
    " - flows [flows[0] is XY flow in RGB, flows[1] is the cell probability in range 0-255 instead of 0.0 to 1.0, flows[2]\n",
    "is Z flow in range 0-255 (if it exists, otherwise zeros),] flows[3] is [dY, dX, cellprob] (or [dZ, dY, dX,\n",
    "cellprob] for 3D), flows[4] is pixel destinations (for internal use)\n",
    " - est_diam : estimated diameter (if run on command line)\n",
    " - zdraw : for each mask, which planes were manually labelled (planes in between manually drawn have interpolated\n",
    "ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import napari\n",
    "\n",
    "segmenations_path = './220624_Fix_Flvw_Emb_PI_Utr647_E02-10X-Z01_downsample_log_seg-cyto.npy'\n",
    "segmentations = np.load(segmenations_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random XY, ZX, and ZY slices from a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful for generating training data for a 3D cellpose model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from tifffile import imread, imwrite\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def get_rand_slices(im: np.ndarray, samples = 3):\n",
    "    ''' \n",
    "    Returns a dictionary of random XY, XZ, and YZ slices from a 3D stack.\n",
    "    Parameters: im - a 3D stack\n",
    "                samples - the number of slices for each dimension to return\n",
    "    Returns:    a dictionary of slices labeled with their dimension and index\n",
    "    '''\n",
    "    assert im.ndim == 3, 'Image must be 3D'\n",
    "    d = {}\n",
    "    z,y,x = im.shape\n",
    "    for n in range(samples):\n",
    "        randyx = randint(0,z)\n",
    "        randzx = randint(0,y)\n",
    "        randzy = randint(0,x)\n",
    "        d[f\"randyx_{randyx}_#{n}\"]=im[randyx]\n",
    "        d[f\"randzx_{randzx}_#{n}\"]=im[:,randzx]\n",
    "        d[f\"randzy_{randzy}_#{n}\"]=im[:,:,randzy]\n",
    "    return d\n",
    "\n",
    "source = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/16bit_scaleZ_sbdl2'\n",
    "dest = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/16bit_scaleZ_sbdl2_random_slices'\n",
    "if not os.path.exists(dest):\n",
    "    os.mkdir(dest)\n",
    "names = [f for f in os.listdir(source) if f.endswith('_16bit_scaleZ_sbdl2_16bit.tif') and not f.startswith('.')]\n",
    "for name in tqdm(names):\n",
    "    im = imread(source +'/'+name)\n",
    "    justname = name.split('.')[0]\n",
    "    slices = get_rand_slices(im, samples = 3)\n",
    "    for k,v in slices.items():\n",
    "        imwrite(f'{dest}/{justname}_{k}.tif',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment dataset by artificially turning xy slices into xz slices by adding 1D Gaussian Blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of synthetic training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "from scipy import ndimage\n",
    "\n",
    "og_path = ''\n",
    "og = imread(og_path)\n",
    "# a Gaussian filter with a standard deviation of 10\n",
    "gauss = ndimage.gaussian_filter1d(og, 7, 0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, dpi=200)\n",
    "ax1.imshow(og, cmap='gray', vmin = 0, vmax=15000)\n",
    "ax2.imshow(gauss, cmap='gray', vmin = 0, vmax=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "target_dir = ''\n",
    "ims = [f for f in os.listdir(target_dir) if f.endswith('tif') and 'yx' in f and not f.startswith('.')]\n",
    "segs = [f for f in os.listdir(target_dir) if f.endswith('.npy') and not f.startswith('.')]\n",
    "\n",
    "# gaussian blur the images:\n",
    "with tqdm(total = len(ims)) as pbar:\n",
    "    for im in ims:\n",
    "        im_path = os.path.join(target_dir, im)\n",
    "        im_data = imread(im_path)\n",
    "        im_data = ndimage.gaussian_filter1d(im_data, 7.5, 0)\n",
    "        imwrite(im_path, im_data)\n",
    "        pbar.update(1)\n",
    "    \n",
    "# rename the images and segmentation files:\n",
    "for f in ims:\n",
    "    base = f.split('.')[0]\n",
    "    newname = base + '_gauss.tif'\n",
    "    os.rename(os.path.join(target_dir, f), os.path.join(target_dir, newname))\n",
    "\n",
    "for s in segs:\n",
    "    base = s.split('_seg.npy')[0]\n",
    "    newname = base + '_gauss_seg.npy'\n",
    "    os.rename(os.path.join(target_dir, s), os.path.join(target_dir, newname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert .npy files to labels and save as tif files\n",
    "\n",
    "This is necessary for training cellpose in GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imwrite\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# dir containing ...tif and ..._seg.npy files\n",
    "raw_dir = '/Volumes/bigData/wholeMount_volDist/CellposeTraining_V2' \n",
    "tifs = [f for f in os.listdir(raw_dir) if f.endswith('.tif') and not f.startswith('.')]\n",
    "segs = [f for f in os.listdir(raw_dir) if f.endswith('.npy') and not f.startswith('.')]\n",
    "\n",
    "# create save directories\n",
    "base_save_dir = '/Volumes/bigData/wholeMount_volDist/CellposeTraining_V2_GD'\n",
    "image_save_dir = os.path.join(base_save_dir, 'Images')\n",
    "mask_save_dir = os.path.join(base_save_dir, 'Masks')\n",
    "if not os.path.exists(image_save_dir):\n",
    "    os.makedirs(image_save_dir)\n",
    "if not os.path.exists(mask_save_dir):\n",
    "    os.makedirs(mask_save_dir)\n",
    "\n",
    "# copy all tifs to the image save dir\n",
    "for f in tifs:\n",
    "    shutil.copy(os.path.join(raw_dir, f), image_save_dir)\n",
    "\n",
    "# extract the masks out of each of the seg files and save as tif to the mask save dir\n",
    "with tqdm(total=len(segs)) as pbar:\n",
    "    for s in segs:\n",
    "        item = np.load(os.path.join(raw_dir, s), allow_pickle=True).item()\n",
    "        masks = item['masks']\n",
    "        save_name = s.split('_seg.npy')[0] + '.tif'\n",
    "        imwrite(os.path.join(mask_save_dir, save_name), masks)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a single npy file into masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imwrite\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# dir containing ...tif and ..._seg.npy files\n",
    "dir = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/16bit_scaleZ_sbdl2'\n",
    "name = '220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Cntrl_E02-Z01_PI_16bit_scaleZ_sbdl2_16bit_seg.npy' \n",
    "\n",
    "# create save directories\n",
    "item = np.load(os.path.join(dir, name), allow_pickle=True).item()\n",
    "masks = item['masks']\n",
    "save_name = name.split('_seg.npy')[0] + '_Masks_Fused.tif'\n",
    "imwrite(os.path.join(dir, save_name), masks, imagej=True, metadata={'axes': 'ZYX'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch process volumes with DoG filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage import filters\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "processing_dir = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/16bit_scaleZ'\n",
    "save_dir = os.path.join(processing_dir, 'sbdl2_processed')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "im_names = [f for f in os.listdir(processing_dir) if f.endswith('.tif') and not f.startswith('.')]\n",
    "\n",
    "with tqdm(total=len(im_names)) as pbar:\n",
    "    for im_name in im_names:\n",
    "        im_base = im_name.split('.')[0]\n",
    "        im = imread(os.path.join(processing_dir, im_name))\n",
    "        sbdl2 = filters.difference_of_gaussians(im, low_sigma=2, high_sigma=128)\n",
    "        # save as 32 bit float\n",
    "        imwrite(os.path.join(save_dir, f'{im_base}_sbdl2.tif'), sbdl2.astype('float32'), imagej=True, metadata={'axes': 'ZYX'})\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove a label and re-save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread, imwrite\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "offending_label_number = 63\n",
    "base = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/U-Net_3D/' \n",
    "name = '220624_Fix_Flvw_Emb_PI_Utr647_E02-10X-Z01_downsample_log_Crop_46-98_Target.tif' \n",
    "masks = imread(os.path.join(base, name))\n",
    "base_name = name.split('.')[0]\n",
    "masks[masks == offending_label_number] = 0\n",
    "imwrite(os.path.join(base, base_name + '_corr.tif'), masks, imagej=True, metadata={'axes': 'ZYX'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn instance segmentation into semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread, imwrite\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_dir = '/Users/bementmbp/Desktop/spindle_UNet/Masks' \n",
    "mask_dir = os.path.join(base_dir, 'Target')\n",
    "#source_dir = os.path.join(base_dir, 'Source')\n",
    "mask_names = [f for f in os.listdir(mask_dir) if f.endswith('.tif') and not f.startswith('.')]\n",
    "\n",
    "save_dir = '/Users/bementmbp/Desktop/spindle_UNet/Masks/binary'\n",
    "mask_save_dir = os.path.join(save_dir, 'Target')\n",
    "#source_save_dir = os.path.join(save_dir, 'Source')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(mask_save_dir):\n",
    "    os.makedirs(mask_save_dir)\n",
    "#if not os.path.exists(source_save_dir):\n",
    "#    os.makedirs(source_save_dir)\n",
    "\n",
    "for mask_name in tqdm(mask_names):\n",
    "    sem = imread(os.path.join(mask_dir, mask_name))\n",
    "    labels = np.unique(sem)\n",
    "    binary = np.zeros(sem.shape)\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            continue\n",
    "        else:\n",
    "            mask = sem == label\n",
    "            mask = ndimage.binary_erosion(mask, iterations = 2)\n",
    "            binary[mask] = 1\n",
    "    imwrite(os.path.join(mask_save_dir, mask_name), binary.astype('uint8'), imagej=True, metadata={'axes': 'ZYX'})\n",
    "    mask_base = mask_name.split('Target.tif')[0]\n",
    "    source_name = mask_base + 'Source.tif'\n",
    "    #source = imread(os.path.join(source_dir, source_name))\n",
    "    #imwrite(os.path.join(source_save_dir, source_name), source[2:-2,2:-2,2:-2].astype('uint16'), imagej=True, metadata={'axes': 'ZYX'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread, imwrite\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_dir = '/Users/bementmbp/Desktop/spindle_UNet/Masks' \n",
    "mask_names = [f for f in os.listdir(base_dir) if f.endswith('.tif') and not f.startswith('.')]\n",
    "\n",
    "save_dir = '/Users/bementmbp/Desktop/spindle_UNet/Masks_cropped'\n",
    "\n",
    "for mask_name in tqdm(mask_names):\n",
    "    im = imread(os.path.join(mask_dir, mask_name))[2:-2,2:-2,2:-2]\n",
    "    imwrite(os.path.join(save_dir, mask_name), binary[2:-2,2:-2,2:-2].astype('uint8'), imagej=True, metadata={'axes': 'ZYX'})\n",
    "    mask_base = mask_name.split('Target.tif')[0]\n",
    "    source_name = mask_base + 'Source.tif'\n",
    "    #source = imread(os.path.join(source_dir, source_name))\n",
    "    #imwrite(os.path.join(source_save_dir, source_name), source[2:-2,2:-2,2:-2].astype('uint16'), imagej=True, metadata={'axes': 'ZYX'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turn instance into multiclass semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread, imwrite\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# source dirs and names\n",
    "base_dir = '/Volumes/bigData/wholeMount_volDist/U-Net_3D_Training' \n",
    "mask_dir = os.path.join(base_dir, 'Target')\n",
    "source_dir = os.path.join(base_dir, 'Source')\n",
    "mask_names = [f for f in os.listdir(mask_dir) if f.endswith('.tif') and not f.startswith('.')]\n",
    "\n",
    "# dest dirs and names\n",
    "save_dir = '/Volumes/bigData/wholeMount_volDist/U-Net_3D_Training_multiclass_8bit_1'\n",
    "mask_save_dir = os.path.join(save_dir, 'Target')\n",
    "source_save_dir = os.path.join(save_dir, 'Source')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(mask_save_dir):\n",
    "    os.makedirs(mask_save_dir)\n",
    "if not os.path.exists(source_save_dir):\n",
    "    os.makedirs(source_save_dir)\n",
    "\n",
    "# make the changes. background pixels are set to 0, edges are 1, cytoplasm is 2\n",
    "for mask_name in tqdm(mask_names):\n",
    "    instance_labels = imread(os.path.join(mask_dir, mask_name))\n",
    "    labels = np.unique(instance_labels)\n",
    "    binary = np.zeros(instance_labels.shape)\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            continue\n",
    "        else:\n",
    "            mask = instance_labels == label\n",
    "            eroded = ndimage.binary_erosion(mask, iterations = 2)\n",
    "            binary[mask] = 1\n",
    "            binary[eroded] = 2\n",
    "    imwrite(os.path.join(mask_save_dir, mask_name), binary[2:-2,2:-2,2:-2].astype('uint8'), imagej=True, metadata={'axes': 'ZYX'})\n",
    "    mask_base = mask_name.split('Target.tif')[0]\n",
    "    source_name = mask_base + 'Source.tif'\n",
    "    source = imread(os.path.join(source_dir, source_name))\n",
    "    imwrite(os.path.join(source_save_dir, source_name), source[2:-2,2:-2,2:-2].astype('uint16'), imagej=True, metadata={'axes': 'ZYX'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert file to 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread, imwrite\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_dir = '/Volumes/bigData/wholeMount_volDist/U-Net_3D_Training_Binary_8bit' \n",
    "mask_dir = os.path.join(base_dir, 'Target')\n",
    "source_dir = os.path.join(base_dir, 'Source')\n",
    "mask_names = [f for f in os.listdir(mask_dir) if f.endswith('.tif') and not f.startswith('.')]\n",
    "\n",
    "for mask_name in tqdm(mask_names):\n",
    "    mask = imread(os.path.join(mask_dir, mask_name))\n",
    "    imwrite(os.path.join(mask_dir, mask_name), mask.astype('uint8'), imagej=True, metadata={'axes': 'ZYX'})\n",
    "    mask_base = mask_name.split('Target.tif')[0]\n",
    "    source_name = mask_base + 'Source.tif'\n",
    "    source = imread(os.path.join(source_dir, source_name))\n",
    "    imwrite(os.path.join(source_dir, source_name), source[1:-1].astype('uint8'), imagej=True, metadata={'axes': 'ZYX'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual annotation of cell cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import napari\n",
    "from tifffile import imread, imwrite\n",
    "from skimage.morphology import label\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "%gui qt \n",
    "viewer=napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/Users/bementmbp/Desktop/test_dir/727' \n",
    "name = 'curr_tub_cube.tif' \n",
    "\n",
    "im = imread(os.path.join(base, name))\n",
    "viewer.add_image(im, name='im', blending='additive')\n",
    "thresh = threshold_otsu(im)\n",
    "print(thresh)\n",
    "mask = label(im > thresh)\n",
    "#viewer.add_labels(mask, name='mask', blending='additive')\n",
    "\n",
    "labels_to_remove = [1,2,3]\n",
    "for l in labels_to_remove:\n",
    "    mask[mask == l] = 0 \n",
    "\n",
    "viewer.add_labels(mask, name='mask', blending='additive')\n",
    "\n",
    "viewer.layers['mask'].save(os.path.join(base, 'thresh_mask.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/napari_tools_menu/__init__.py:168: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.5.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  self.tools_menu = ToolsMenu(self, self.qt_viewer.viewer)\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/jupyter_client/threaded.py\", line 121, in _handle_recv\n",
      "    msg_list = self.ioloop._asyncio_event_loop.run_until_complete(get_msg(future_msg))\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 623, in run_until_complete\n",
      "    self._check_running()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 585, in _check_running\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot run the event loop while another loop is running\n",
      "ERROR:tornado.general:Uncaught exception in zmqstream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/jupyter_client/threaded.py\", line 121, in _handle_recv\n",
      "    msg_list = self.ioloop._asyncio_event_loop.run_until_complete(get_msg(future_msg))\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 623, in run_until_complete\n",
      "    self._check_running()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 585, in _check_running\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot run the event loop while another loop is running\n",
      "Exception in callback BaseAsyncIOLoop._handle_events(170, 1)\n",
      "handle: <Handle BaseAsyncIOLoop._handle_events(170, 1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 189, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/jupyter_client/threaded.py\", line 121, in _handle_recv\n",
      "    msg_list = self.ioloop._asyncio_event_loop.run_until_complete(get_msg(future_msg))\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 623, in run_until_complete\n",
      "    self._check_running()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 585, in _check_running\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot run the event loop while another loop is running\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import napari\n",
    "from tifffile import imread, imwrite\n",
    "from skimage.morphology import label\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "deletionviewer=napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/threading.py:239: RuntimeWarning: coroutine 'get_msg' was never awaited\n",
      "  self._release_save = lock._release_save\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/bementmbp/Desktop/Scripts/volume-distribution/classifier/good_label_folders/82/thresh_mask.tif']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskpath = '/Users/bementmbp/Desktop/Scripts/volume-distribution/classifier/good_label_folders/82/thresh_mask.tif'\n",
    "\n",
    "mask = imread(maskpath)\n",
    "\n",
    "mask = np.zeros(mask.shape).astype('uint8')\n",
    "deletionviewer.add_labels(mask, name='mask', blending='additive')\n",
    "deletionviewer.layers['mask'].save(maskpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# automatically export volumes to images and labels directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import napari\n",
    "from tifffile import imread, imwrite\n",
    "from skimage.morphology import label\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "%gui qt \n",
    "viewer=napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/Users/bementmbp/Desktop/autosave2/Exp_E10' \n",
    "save_dir = '/Users/bementmbp/Desktop/iter_three'   \n",
    "\n",
    "good_examples = [104, 123, 142, 191]\n",
    "\n",
    "for ex in good_examples:\n",
    "    ex_path = os.path.join(main_dir, str(ex))\n",
    "    raw_tub = imread(os.path.join(ex_path, 'curr_tub_raw_cube.tif'))\n",
    "    filt_tub = imread(os.path.join(ex_path, 'curr_tub_cube.tif'))\n",
    "    mask = imread(os.path.join(ex_path, 'thresh_mask.tif'))\n",
    "\n",
    "    image_save_dir = os.path.join(save_dir, 'Images')\n",
    "    if not os.path.exists(image_save_dir):\n",
    "        os.makedirs(image_save_dir)\n",
    "    \n",
    "    mask_save_dir = os.path.join(save_dir, 'Masks')\n",
    "    if not os.path.exists(mask_save_dir):\n",
    "        os.makedirs(mask_save_dir)\n",
    "    \n",
    "    imwrite(os.path.join(image_save_dir, f'curr_tub_raw_cube_{ex}.tif'), raw_tub)\n",
    "    imwrite(os.path.join(image_save_dir, f'curr_tub_cube_{ex}.tif'), filt_tub)\n",
    "\n",
    "    label_nums = np.unique(mask)\n",
    "    binary = np.zeros(mask.shape, dtype='uint8')\n",
    "\n",
    "    for label in label_nums:\n",
    "        if label == 0:\n",
    "            continue\n",
    "        else:\n",
    "            label_mask = mask == label\n",
    "            binary[label_mask] = 1\n",
    "    \n",
    "    imwrite(os.path.join(mask_save_dir, f'curr_tub_raw_cube_{ex}.tif'), binary)\n",
    "    imwrite(os.path.join(mask_save_dir, f'curr_tub_cube_{ex}.tif'), binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main workflow below!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering low quality segmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import napari\n",
    "from tifffile import imread, imwrite\n",
    "from skimage import morphology\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_multiotsu\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import clear_border\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "from skimage import filters\n",
    "from skimage.morphology import label\n",
    "from skimage.morphology import binary_erosion\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import regionprops\n",
    "from scipy import spatial\n",
    "import vg # vector-geometry and linear-algebra toolbelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/napari_tools_menu/__init__.py:168: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.5.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  self.tools_menu = ToolsMenu(self, self.qt_viewer.viewer)\n"
     ]
    }
   ],
   "source": [
    "%gui qt \n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data and define useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter poor quality labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:31<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no spindles detected in mask 81\n",
      "no spindles detected in mask 83\n",
      "no spindles detected in mask 97\n",
      "no spindles detected in mask 102\n",
      "no spindles detected in mask 107\n",
      "no spindles detected in mask 120\n",
      "no spindles detected in mask 122\n",
      "no spindles detected in mask 125\n",
      "no spindles detected in mask 126\n",
      "no spindles detected in mask 127\n",
      "more than one region remaining!\n",
      "no spindles detected in mask 149\n",
      "no spindles detected in mask 156\n",
      "no spindles detected in mask 163\n",
      "no spindles detected in mask 164\n",
      "no spindles detected in mask 167\n",
      "no spindles detected in mask 168\n",
      "no spindles detected in mask 180\n",
      "no spindles detected in mask 184\n",
      "no spindles detected in mask 190\n",
      "no spindles detected in mask 192\n",
      "no spindles detected in mask 194\n",
      "no spindles detected in mask 199\n",
      "no spindles detected in mask 200\n",
      "no spindles detected in mask 203\n",
      "no spindles detected in mask 205\n",
      "no spindles detected in mask 207\n",
      "no spindles detected in mask 210\n",
      "no spindles detected in mask 211\n",
      "no spindles detected in mask 221\n",
      "no spindles detected in mask 222\n",
      "no spindles detected in mask 224\n",
      "no spindles detected in mask 237\n",
      "no spindles detected in mask 239\n",
      "no spindles detected in mask 241\n",
      "no spindles detected in mask 244\n",
      "no spindles detected in mask 245\n",
      "no spindles detected in mask 246\n",
      "no spindles detected in mask 252\n",
      "no spindles detected in mask 256\n",
      "no spindles detected in mask 257\n",
      "no spindles detected in mask 261\n",
      "more than one region remaining!\n",
      "no spindles detected in mask 263\n",
      "no spindles detected in mask 267\n",
      "no spindles detected in mask 270\n",
      "no spindles detected in mask 271\n",
      "no spindles detected in mask 272\n",
      "no spindles detected in mask 276\n",
      "no spindles detected in mask 279\n",
      "no spindles detected in mask 280\n",
      "no spindles detected in mask 283\n",
      "no spindles detected in mask 286\n",
      "no spindles detected in mask 289\n",
      "no spindles detected in mask 290\n",
      "no spindles detected in mask 292\n",
      "no spindles detected in mask 295\n",
      "no spindles detected in mask 296\n",
      "no spindles detected in mask 297\n",
      "no spindles detected in mask 300\n",
      "no spindles detected in mask 303\n",
      "no spindles detected in mask 304\n",
      "no spindles detected in mask 306\n",
      "no spindles detected in mask 308\n",
      "no spindles detected in mask 309\n",
      "finished with 06\n"
     ]
    }
   ],
   "source": [
    "from skimage.morphology import label\n",
    "\n",
    "base_dir = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/16bit_scaleZ_sbdl2' \n",
    "\n",
    "cntrl = False\n",
    "emb_nums = ['06']\n",
    "for emb_num in emb_nums:\n",
    "\n",
    "    if cntrl == True:\n",
    "        emb_type = 'Cntrl'\n",
    "        segmentations_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_PI_16bit_scaleZ_sbdl2_16bit_seg.npy'\n",
    "        dog_tub_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_Tub_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "        raw_tub_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_Tub_16bit_scaleZ.tif'\n",
    "        pi_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_PI_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "\n",
    "        masks = np.load(os.path.join(base_dir, segmentations_name), allow_pickle=True).item()['masks']\n",
    "        tub = imread(os.path.join(base_dir, dog_tub_name))\n",
    "        tub_raw = imread(os.path.join(base_dir, raw_tub_name))\n",
    "        pi = imread(os.path.join(base_dir, pi_name))\n",
    "\n",
    "    if cntrl == False:\n",
    "        emb_type = 'Exp'\n",
    "        segmentations_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_PI_16bit_scaleZ_sbdl2_16bit_seg.npy'\n",
    "        dog_tub_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_Tub_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "        raw_tub_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_Tub_16bit_scaleZ.tif'\n",
    "        pi_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_E{emb_num}-Z01_PI_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "\n",
    "        masks = np.load(os.path.join(base_dir, segmentations_name), allow_pickle=True).item()['masks']\n",
    "        tub = imread(os.path.join(base_dir, dog_tub_name))\n",
    "        tub_raw = imread(os.path.join(base_dir, raw_tub_name))\n",
    "        pi = imread(os.path.join(base_dir, pi_name))\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    minimum_size = 250000\n",
    "    maximum_size = 1000000\n",
    "    minimum_density = 0.21\n",
    "\n",
    "    filtered_masks = clear_border(masks)\n",
    "    filtered_masks = morphology.remove_small_objects(filtered_masks, min_size=minimum_size, connectivity=1)\n",
    "    filtered_masks = remove_large_objects(filtered_masks, max_size=maximum_size)\n",
    "    remaining_labels = [label for label in np.unique(filtered_masks) if label != 0]\n",
    "    print('Calculating point clouds...')\n",
    "    label_pcs = [return_points(masks, label_ID) for label_ID in tqdm(remaining_labels)]\n",
    "    densities = [find_label_density(pc) for pc in label_pcs]\n",
    "\n",
    "    for ind, id in enumerate(remaining_labels):\n",
    "        if id in remaining_labels and densities[ind] < minimum_density:\n",
    "            filtered_masks[filtered_masks == id] = 0\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "\n",
    "    final_labels = [label for label in np.unique(filtered_masks) if label != 0]\n",
    "    main_save_dir = '/Users/bementmbp/Desktop/autosave2' \n",
    "    results = {}\n",
    "\n",
    "    def calculate_geometries(curr_mask_id):\n",
    "\n",
    "        # start with a fresh slate\n",
    "        wipe_layers()\n",
    "        results[curr_mask_id] = []\n",
    "        curr_mask = masks == curr_mask_id\n",
    "\n",
    "        # get the coordinate of the bounding cube for the current mask ID. Apply it to the labels and images\n",
    "        cube_dims = get_cube(masks, curr_mask_id)\n",
    "        cubed_label = apply_cube(curr_mask, cube_dims)\n",
    "        cubed_tub = apply_cube(tub, cube_dims)\n",
    "        cubed_tub_raw = apply_cube(tub_raw, cube_dims)\n",
    "        cubed_PI = apply_cube(pi, cube_dims)\n",
    "\n",
    "        # erode the mask to eliminate some cortical signal\n",
    "        eroded_mask = binary_erosion(cubed_label, footprint=np.ones((3, 3, 3)))\n",
    "        for i in range(10):\n",
    "            eroded_mask = binary_erosion(eroded_mask)\n",
    "\n",
    "        # get the tubulin signal from the remaining region and define an Otsu threshold\n",
    "        remaining_tub = np.zeros(shape=cubed_label.shape)\n",
    "        remaining_tub[eroded_mask] = cubed_tub[eroded_mask]\n",
    "        remaining_vals = cubed_tub[eroded_mask].ravel()\n",
    "        thresh_val = threshold_otsu(remaining_vals)\n",
    "        thresh_mask = label(remaining_tub > thresh_val)\n",
    "\n",
    "        # filter and labels smaller or larger than the mimum and maximum expected label sizes\n",
    "        min_thrsh_size = 500\n",
    "        max_thrsh_size = 5000\n",
    "        num_tub_labels_b4_filter = [label for label in np.unique(thresh_mask) if label != 0]\n",
    "\n",
    "        if len(num_tub_labels_b4_filter) == 0:\n",
    "            # no filtering required if we already have no labels\n",
    "            print(f'no spindles detected in label {curr_mask_id}')\n",
    "            return\n",
    "\n",
    "        if len(num_tub_labels_b4_filter) == 1:\n",
    "            # remove_small_objects will complain about this, so let's just pass it by remove_large_objects\n",
    "            thresh_mask = remove_large_objects(thresh_mask, max_size=max_thrsh_size)\n",
    "\n",
    "        if len(num_tub_labels_b4_filter) > 1:\n",
    "            # here it's likely that we have both small and large objects coontaminating\n",
    "            thresh_mask = morphology.remove_small_objects(thresh_mask, min_size=min_thrsh_size, connectivity=1)\n",
    "            thresh_mask = remove_large_objects(thresh_mask, max_size=max_thrsh_size)\n",
    "\n",
    "        # get the number of labels after filtering\n",
    "        remaining_labels = [label for label in np.unique(thresh_mask) if label != 0]\n",
    "        if len(remaining_labels) > 1:\n",
    "            print('more than one region remaining!')\n",
    "            return\n",
    "\n",
    "        if len(remaining_labels) == 0:\n",
    "            print(f'no spindles detected in mask {curr_mask_id}')\n",
    "            return\n",
    "\n",
    "        # get the spindle coordinates and centroid\n",
    "        spindle_label_ID = remaining_labels[0]\n",
    "        '''\n",
    "        record the spindle properties so we can make sure we're not analyzing any crazy shapes\n",
    "        '''\n",
    "        spindle_coords = np.column_stack(np.where(thresh_mask == spindle_label_ID))\n",
    "        spindle_centroid = spindle_coords.mean(axis=0)\n",
    "        spindle_long_vect, spindle_long_line = get_long_axis(thresh_mask)\n",
    "\n",
    "        # get the mask coordinates, centroid, and long axis\n",
    "        mask_coords = np.column_stack(np.where(cubed_label == True))\n",
    "        cell_centroid = mask_coords.mean(axis=0)\n",
    "        cell_long_vect, cell_long_line = get_long_axis(cubed_label)\n",
    "\n",
    "        # get the distance between the centroids, and the angles between the long axes\n",
    "        dist = spatial.distance.euclidean(cell_centroid, spindle_centroid)\n",
    "        ang = vg.angle(spindle_long_vect, cell_long_vect)\n",
    "        results[curr_mask_id].append(dist)\n",
    "        results[curr_mask_id].append(ang)\n",
    "\n",
    "        # make a save directory for this mask\n",
    "        if cntrl:\n",
    "            emb_save_dir = os.path.join(main_save_dir, f'Cntrl_E{emb_num}')\n",
    "        else:\n",
    "            emb_save_dir = os.path.join(main_save_dir, f'Exp_E{emb_num}')\n",
    "        if not os.path.exists(emb_save_dir):\n",
    "            os.makedirs(emb_save_dir)\n",
    "        \n",
    "        save_dir = os.path.join(emb_save_dir, f'{curr_mask_id}')\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "\n",
    "        # populate the viewer \n",
    "        viewer.add_labels(eroded_mask, name='eroded_mask', blending='additive', visible=False)\n",
    "        viewer.add_labels(cubed_label, name='curr_mask_cube', blending='additive')\n",
    "        viewer.add_image(cubed_tub, name='curr_tub_cube', blending='additive', visible=False)\n",
    "        viewer.add_image(cubed_tub_raw, name='curr_tub_raw_cube', blending='additive', visible=False)\n",
    "        viewer.add_image(cubed_PI, name='curr_PI_cube', blending='additive', visible=False)\n",
    "        viewer.add_labels(thresh_mask, name='thresh_mask', blending='additive')\n",
    "        viewer.add_points(cell_centroid, name='spindle centroid', face_color='magenta', blending='additive')\n",
    "        viewer.add_points(spindle_centroid, name='spindle centroid', face_color='green', blending='additive')\n",
    "        viewer.add_shapes(cell_long_line, shape_type='line', name='cell long axis', edge_color='red', blending='additive')\n",
    "        viewer.add_shapes(spindle_long_line, shape_type='line', name='spindle long axis', edge_color='blue', blending='additive')\n",
    "\n",
    "        images_and_layers = ['curr_mask_cube',\n",
    "                            'curr_tub_cube',\n",
    "                            'curr_tub_raw_cube',\n",
    "                            'curr_PI_cube',\n",
    "                            'eroded_mask',\n",
    "                            'thresh_mask']\n",
    "\n",
    "        # save the tif compatible layers as tifs\n",
    "        for item in images_and_layers:\n",
    "            viewer.layers[item].save(os.path.join(save_dir, item + '.tif'))\n",
    "\n",
    "        # save the arrays as txt files\n",
    "        np.savetxt(os.path.join(save_dir, 'spindle_centroid.txt'), spindle_centroid)\n",
    "        np.savetxt(os.path.join(save_dir, 'cell_centroid.txt'), cell_centroid)\n",
    "        np.savetxt(os.path.join(save_dir, 'spindle_long_axis.txt'), spindle_long_line)\n",
    "\n",
    "    for curr_mask_id in final_labels:\n",
    "        calculate_geometries(curr_mask_id)\n",
    "    \n",
    "    print(f'finished with {emb_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "727 is bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting an error with one of the masks in Exp E01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 640, 640)\n",
      "(396, 640, 640)\n",
      "(396, 640, 640)\n",
      "(396, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/16bit_scaleZ_sbdl2' \n",
    "\n",
    "segmentations_name = '220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Exp_E06-Z01_PI_16bit_scaleZ_sbdl2_16bit_seg.npy'\n",
    "dog_tub_name = '220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Exp_E06-Z01_Tub_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "raw_tub_name = '220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Exp_E06-Z01_Tub_16bit_scaleZ.tif'\n",
    "pi_name = '220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Exp_E06-Z01_PI_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "\n",
    "masks = np.load(os.path.join(base_dir, segmentations_name), allow_pickle=True).item()['masks']\n",
    "tub = imread(os.path.join(base_dir, dog_tub_name))\n",
    "tub_raw = imread(os.path.join(base_dir, raw_tub_name))\n",
    "pi = imread(os.path.join(base_dir, pi_name))\n",
    "\n",
    "print(masks.shape)\n",
    "print(tub.shape)\n",
    "print(tub_raw.shape)\n",
    "print(pi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/napari_tools_menu/__init__.py:168: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.5.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  self.tools_menu = ToolsMenu(self, self.qt_viewer.viewer)\n"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/_future.py:466: RuntimeWarning: coroutine 'get_msg' was never awaited\n",
      "  if not self._shadow_sock.get(EVENTS) & POLLIN:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:30<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "minimum_size = 250000\n",
    "maximum_size = 1000000\n",
    "minimum_density = 0.21\n",
    "\n",
    "filtered_masks = clear_border(masks)\n",
    "filtered_masks = morphology.remove_small_objects(filtered_masks, min_size=minimum_size, connectivity=1)\n",
    "filtered_masks = remove_large_objects(filtered_masks, max_size=maximum_size)\n",
    "remaining_labels = [label for label in np.unique(filtered_masks) if label != 0]\n",
    "print('Calculating point clouds...')\n",
    "label_pcs = [return_points(masks, label_ID) for label_ID in tqdm(remaining_labels)]\n",
    "densities = [find_label_density(pc) for pc in label_pcs]\n",
    "\n",
    "for ind, id in enumerate(remaining_labels):\n",
    "    if id in remaining_labels and densities[ind] < minimum_density:\n",
    "        filtered_masks[filtered_masks == id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = [label for label in np.unique(filtered_masks) if label != 0]\n",
    "main_save_dir = '/Users/bementmbp/Desktop/autosave2' \n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "5279\n",
      "(148, 92, 107)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb Cell 60\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=118'>119</a>\u001b[0m     np\u001b[39m.\u001b[39msavetxt(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_dir, \u001b[39m'\u001b[39m\u001b[39mspindle_long_axis.txt\u001b[39m\u001b[39m'\u001b[39m), spindle_long_line)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=120'>121</a>\u001b[0m \u001b[39mfor\u001b[39;00m curr_mask_id \u001b[39min\u001b[39;00m final_labels:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=121'>122</a>\u001b[0m     calculate_geometries(curr_mask_id)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=123'>124</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfinished with \u001b[39m\u001b[39m{\u001b[39;00memb_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb Cell 60\u001b[0m in \u001b[0;36mcalculate_geometries\u001b[0;34m(curr_mask_id)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=27'>28</a>\u001b[0m blah \u001b[39m=\u001b[39m remaining_tub \u001b[39m>\u001b[39m thresh_val\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(blah\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=29'>30</a>\u001b[0m thresh_mask \u001b[39m=\u001b[39m label()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=31'>32</a>\u001b[0m \u001b[39m# filter and labels smaller or larger than the mimum and maximum expected label sizes\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/Methods.ipynb#ch0000063?line=32'>33</a>\u001b[0m min_thrsh_size \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int32' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_geometries(curr_mask_id):\n",
    "\n",
    "    print(curr_mask_id)\n",
    "\n",
    "    # start with a fresh slate\n",
    "    wipe_layers()\n",
    "    results[curr_mask_id] = []\n",
    "    curr_mask = masks == curr_mask_id\n",
    "\n",
    "    # get the coordinate of the bounding cube for the current mask ID. Apply it to the labels and images\n",
    "    cube_dims = get_cube(masks, curr_mask_id)\n",
    "    cubed_label = apply_cube(curr_mask, cube_dims)\n",
    "    cubed_tub = apply_cube(tub, cube_dims)\n",
    "    cubed_tub_raw = apply_cube(tub_raw, cube_dims)\n",
    "    cubed_PI = apply_cube(pi, cube_dims)\n",
    "\n",
    "    # erode the mask to eliminate some cortical signal\n",
    "    eroded_mask = binary_erosion(cubed_label, footprint=np.ones((3, 3, 3)))\n",
    "    for i in range(10):\n",
    "        eroded_mask = binary_erosion(eroded_mask)\n",
    "\n",
    "    # get the tubulin signal from the remaining region and define an Otsu threshold\n",
    "    remaining_tub = np.zeros(shape=cubed_label.shape)\n",
    "    remaining_tub[eroded_mask] = cubed_tub[eroded_mask]\n",
    "    remaining_vals = cubed_tub[eroded_mask].ravel()\n",
    "    thresh_val = threshold_otsu(remaining_vals)\n",
    "    print(thresh_val)\n",
    "    blah = remaining_tub > thresh_val\n",
    "    print(blah.shape)\n",
    "    thresh_mask = label()\n",
    "\n",
    "    # filter and labels smaller or larger than the mimum and maximum expected label sizes\n",
    "    min_thrsh_size = 500\n",
    "    max_thrsh_size = 5000\n",
    "    num_tub_labels_b4_filter = [label for label in np.unique(thresh_mask) if label != 0]\n",
    "\n",
    "    if len(num_tub_labels_b4_filter) == 0:\n",
    "        # no filtering required if we already have no labels\n",
    "        print(f'no spindles detected in label {curr_mask_id}')\n",
    "        return\n",
    "\n",
    "    if len(num_tub_labels_b4_filter) == 1:\n",
    "        # remove_small_objects will complain about this, so let's just pass it by remove_large_objects\n",
    "        thresh_mask = remove_large_objects(thresh_mask, max_size=max_thrsh_size)\n",
    "\n",
    "    if len(num_tub_labels_b4_filter) > 1:\n",
    "        # here it's likely that we have both small and large objects coontaminating\n",
    "        thresh_mask = morphology.remove_small_objects(thresh_mask, min_size=min_thrsh_size, connectivity=1)\n",
    "        thresh_mask = remove_large_objects(thresh_mask, max_size=max_thrsh_size)\n",
    "\n",
    "    # get the number of labels after filtering\n",
    "    remaining_labels = [label for label in np.unique(thresh_mask) if label != 0]\n",
    "    if len(remaining_labels) > 1:\n",
    "        print('more than one region remaining!')\n",
    "        return\n",
    "\n",
    "    if len(remaining_labels) == 0:\n",
    "        print(f'no spindles detected in mask {curr_mask_id}')\n",
    "        return\n",
    "\n",
    "    # get the spindle coordinates and centroid\n",
    "    spindle_label_ID = remaining_labels[0]\n",
    "    '''\n",
    "    record the spindle properties so we can make sure we're not analyzing any crazy shapes\n",
    "    '''\n",
    "    spindle_coords = np.column_stack(np.where(thresh_mask == spindle_label_ID))\n",
    "    spindle_centroid = spindle_coords.mean(axis=0)\n",
    "    spindle_long_vect, spindle_long_line = get_long_axis(thresh_mask)\n",
    "\n",
    "    # get the mask coordinates, centroid, and long axis\n",
    "    mask_coords = np.column_stack(np.where(cubed_label == True))\n",
    "    cell_centroid = mask_coords.mean(axis=0)\n",
    "    cell_long_vect, cell_long_line = get_long_axis(cubed_label)\n",
    "\n",
    "    # get the distance between the centroids, and the angles between the long axes\n",
    "    dist = spatial.distance.euclidean(cell_centroid, spindle_centroid)\n",
    "    ang = vg.angle(spindle_long_vect, cell_long_vect)\n",
    "    results[curr_mask_id].append(dist)\n",
    "    results[curr_mask_id].append(ang)\n",
    "\n",
    "    # make a save directory for this mask\n",
    "    if cntrl:\n",
    "        emb_save_dir = os.path.join(main_save_dir, f'Cntrl_E{emb_num}')\n",
    "    else:\n",
    "        emb_save_dir = os.path.join(main_save_dir, f'Exp_E{emb_num}')\n",
    "    if not os.path.exists(emb_save_dir):\n",
    "        os.makedirs(emb_save_dir)\n",
    "    \n",
    "    save_dir = os.path.join(emb_save_dir, f'{curr_mask_id}')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    # populate the viewer \n",
    "    viewer.add_labels(eroded_mask, name='eroded_mask', blending='additive', visible=False)\n",
    "    viewer.add_labels(cubed_label, name='curr_mask_cube', blending='additive')\n",
    "    viewer.add_image(cubed_tub, name='curr_tub_cube', blending='additive', visible=False)\n",
    "    viewer.add_image(cubed_tub_raw, name='curr_tub_raw_cube', blending='additive', visible=False)\n",
    "    viewer.add_image(cubed_PI, name='curr_PI_cube', blending='additive', visible=False)\n",
    "    viewer.add_labels(thresh_mask, name='thresh_mask', blending='additive')\n",
    "    viewer.add_points(cell_centroid, name='spindle centroid', face_color='magenta', blending='additive')\n",
    "    viewer.add_points(spindle_centroid, name='spindle centroid', face_color='green', blending='additive')\n",
    "    viewer.add_shapes(cell_long_line, shape_type='line', name='cell long axis', edge_color='red', blending='additive')\n",
    "    viewer.add_shapes(spindle_long_line, shape_type='line', name='spindle long axis', edge_color='blue', blending='additive')\n",
    "\n",
    "    images_and_layers = ['curr_mask_cube',\n",
    "                        'curr_tub_cube',\n",
    "                        'curr_tub_raw_cube',\n",
    "                        'curr_PI_cube',\n",
    "                        'eroded_mask',\n",
    "                        'thresh_mask']\n",
    "\n",
    "    # save the tif compatible layers as tifs\n",
    "    for item in images_and_layers:\n",
    "        viewer.layers[item].save(os.path.join(save_dir, item + '.tif'))\n",
    "\n",
    "    # save the arrays as txt files\n",
    "    np.savetxt(os.path.join(save_dir, 'spindle_centroid.txt'), spindle_centroid)\n",
    "    np.savetxt(os.path.join(save_dir, 'cell_centroid.txt'), cell_centroid)\n",
    "    np.savetxt(os.path.join(save_dir, 'spindle_long_axis.txt'), spindle_long_line)\n",
    "\n",
    "for curr_mask_id in final_labels:\n",
    "    calculate_geometries(curr_mask_id)\n",
    "\n",
    "print(f'finished with {emb_num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/napari_tools_menu/__init__.py:168: FutureWarning: Public access to Window.qt_viewer is deprecated and will be removed in\n",
      "v0.5.0. It is considered an \"implementation detail\" of the napari\n",
      "application, not part of the napari viewer model. If your use case\n",
      "requires access to qt_viewer, please open an issue to discuss.\n",
      "  self.tools_menu = ToolsMenu(self, self.qt_viewer.viewer)\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/jupyter_client/threaded.py\", line 121, in _handle_recv\n",
      "    msg_list = self.ioloop._asyncio_event_loop.run_until_complete(get_msg(future_msg))\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 623, in run_until_complete\n",
      "    self._check_running()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 585, in _check_running\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot run the event loop while another loop is running\n",
      "ERROR:tornado.general:Uncaught exception in zmqstream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/jupyter_client/threaded.py\", line 121, in _handle_recv\n",
      "    msg_list = self.ioloop._asyncio_event_loop.run_until_complete(get_msg(future_msg))\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 623, in run_until_complete\n",
      "    self._check_running()\n",
      "  File \"/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/asyncio/base_events.py\", line 585, in _check_running\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot run the event loop while another loop is running\n"
     ]
    }
   ],
   "source": [
    "methodsViewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/concurrent/futures/thread.py:162: RuntimeWarning: coroutine 'get_msg' was never awaited\n",
      "  with self._shutdown_lock, _global_shutdown_lock:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Image layer 'deftub' at 0x7ff1da754940>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage import data\n",
    "import elasticdeform\n",
    "\n",
    "mask = imread('/Users/bementmbp/Desktop/autosave2/Exp_E10/169/thresh_mask.tif')\n",
    "tub = imread('/Users/bementmbp/Desktop/autosave2/Exp_E10/169/curr_tub_raw_cube.tif')\n",
    "methodsViewer.add_labels(mask, name='mask', blending='additive')\n",
    "methodsViewer.add_image(tub, name='tub', blending='additive')\n",
    "\n",
    "sigma_= 2 #deformation parameter\n",
    "points_= 2 #deformation parameter\n",
    "\n",
    "[defmask, deftub] = elasticdeform.deform_random_grid([mask, tub], sigma=sigma_, points=points_)\n",
    "methodsViewer.add_labels(defmask, name='defmask', blending='additive')\n",
    "methodsViewer.add_image(deftub, name='deftub', blending='additive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propsviewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "import pandas as pd\n",
    "\n",
    "test_dir = '/Users/bementmbp/Desktop/autosave2/Exp_E02/46' \n",
    "mask_cube_name = 'curr_mask_cube.tif'\n",
    "tub_cube_name = 'curr_tub_cube.tif'\n",
    "\n",
    "\n",
    "my_labels = label(imread('/Users/bementmbp/Desktop/iter_4/Masks/curr_tub_raw_cube_46.tif'))\n",
    "\n",
    "regions = regionprops(my_labels)\n",
    "props = regionprops_table(my_labels, properties=('area',\n",
    "                                                 'axis_major_length',\n",
    "                                                 'axis_minor_length'))\n",
    "props_df = pd.DataFrame(props)\n",
    "props_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('napari_apoc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9332436963653b5df92bf2f53404d912d66b2e5a7a0f130fd2cb37eb34a8db49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
