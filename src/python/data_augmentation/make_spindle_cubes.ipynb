{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vg\n",
    "import time\n",
    "import joblib\n",
    "import napari\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "from skimage import filters\n",
    "from skimage import morphology\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from tifffile import imread, imwrite\n",
    "from skimage.morphology import label\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.morphology import binary_erosion\n",
    "from skimage.measure import regionprops, regionprops_table\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def wipe_layers(viewer_name) -> None:\n",
    "    '''\n",
    "    Delete all layers in the viewer objected\n",
    "    '''\n",
    "    layers = viewer_name.layers\n",
    "    while len(layers) > 0:\n",
    "        layers.remove(layers[0])\n",
    "\n",
    "def remove_large_objects(labels_array: np.ndarray, max_size: int) -> np.ndarray:\n",
    "    ''' \n",
    "    Remove all objects in a mask above a specific threshold\n",
    "    '''\n",
    "    out = np.copy(labels_array)\n",
    "    component_sizes = np.bincount(labels_array.ravel()) # count the number of pixels in different labels\n",
    "    too_big = component_sizes > max_size\n",
    "    too_big_mask = too_big[labels_array]\n",
    "    out[too_big_mask] = 0\n",
    "    return out\n",
    "\n",
    "def return_points(labels_array: np.ndarray, label_ID: int) -> np.ndarray:\n",
    "    '''\n",
    "    Return the points in a mask that belong to a specific label\n",
    "    ---\n",
    "    Parameters:\n",
    "    labels_array: np.ndarray an ndArray of labels\n",
    "    label_ID: int the label ID of the label whos points you want to calculate\n",
    "    ---\n",
    "    Returns:\n",
    "    points: np.ndarray an ndArray of shape (n,3) where n is the number of points in the label\n",
    "    and dim1 is the x,y,z coordinates of the points\n",
    "    '''\n",
    "    points = np.column_stack(np.where(labels_array == label_ID))\n",
    "    return points\n",
    "\n",
    "def find_label_density(label_points: np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculate the bounding box for a point cloud and return the density of points in the bounding box\n",
    "    ---\n",
    "    Parameters:\n",
    "    label_points: np.ndarray the array point coordinates for a given label\n",
    "    ---\n",
    "    Returns:\n",
    "    np.nan if the label is 0, or if the label has no length\n",
    "    density (float) the number of points in the label divided by the volume of the bounding box\n",
    "    '''\n",
    "\n",
    "    x = label_points.T[0]\n",
    "    y = label_points.T[1]\n",
    "    z = label_points.T[2]\n",
    "    num_points = len(x)\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    y_min = np.min(y)\n",
    "    y_max = np.max(y)\n",
    "    z_min = np.min(z)\n",
    "    z_max = np.max(z)\n",
    "    # add 1 to prevent division by 0\n",
    "    x_range = (x_max - x_min) + 1\n",
    "    y_range = (y_max - y_min) + 1\n",
    "    z_range = (z_max - z_min) + 1\n",
    "    vol = x_range * y_range * z_range\n",
    "    density = num_points / vol\n",
    "    return density\n",
    "\n",
    "def print_label_props(source: np.ndarray, label_num: int) -> None:\n",
    "    '''\n",
    "    Print the properties of a label in a mask\n",
    "    ---\n",
    "    Parameters:\n",
    "    source: np.ndarray the mask containing the label\n",
    "    label_num: int the label number of the label you want to print the properties of\n",
    "    ---\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    label_points = return_points(source, label_num)\n",
    "    density = find_label_density(label_points)\n",
    "    size = label_points.shape[0]\n",
    "    print(f'Label {label_num} has:')\n",
    "    print(f'{size:,} points.')\n",
    "    print(f'density of {round(density,4):,}')\n",
    "\n",
    "def get_cube(source: np.ndarray, label_num: int) -> np.ndarray:\n",
    "    '''\n",
    "    Return a cube of the label in a mask\n",
    "    ---\n",
    "    Parameters:\n",
    "    source: np.ndarray the mask containing the label\n",
    "    label_num: int the label number of the label you want isolate\n",
    "    ---\n",
    "    Returns:\n",
    "    cube: np.ndarray the cube of the label\n",
    "    '''\n",
    "    label_points = return_points(source, label_num)\n",
    "    x = label_points.T[0]\n",
    "    y = label_points.T[1]\n",
    "    z = label_points.T[2]\n",
    "    x_min = np.min(x) - 1\n",
    "    x_max = np.max(x) + 2\n",
    "    y_min = np.min(y) - 1\n",
    "    y_max = np.max(y) + 2\n",
    "    z_min = np.min(z) - 1\n",
    "    z_max = np.max(z) + 2\n",
    "    #cube = source[x_min:x_max+1, y_min:y_max+1, z_min:z_max+1]\n",
    "    return x_min, x_max, y_min, y_max, z_min, z_max\n",
    "\n",
    "def apply_cube(source: np.ndarray, cube: tuple) -> np.ndarray:\n",
    "    '''\n",
    "    Crop an ndArray with a cube\n",
    "    ---\n",
    "    Parameters:\n",
    "    source: np.ndarray the array to crop\n",
    "    cube: tuple containing the x_min, x_max, y_min, y_max, z_min, z_max\n",
    "    ---\n",
    "    Returns:\n",
    "    out: np.ndarray array with the cube applied\n",
    "    '''\n",
    "    x_min, x_max, y_min, y_max, z_min, z_max = cube\n",
    "    out = source[x_min:x_max, y_min:y_max, z_min:z_max]\n",
    "    return out\n",
    "\n",
    "def get_long_axis(cubed_label: np.ndarray, line_length = 75):\n",
    "    '''\n",
    "    Get the longest axis of an cubed_label\n",
    "    ---\n",
    "    Parameters:\n",
    "    cubed_label: np.ndarray the cubed_label to get the longest axis of\n",
    "    ---\n",
    "    Returns:\n",
    "    vv[0]: vector of the longest axis\n",
    "    linepts: np.ndarray the points of the longest axis\n",
    "    '''\n",
    "    if cubed_label.dtype == 'bool':\n",
    "        coords = np.column_stack(np.where(cubed_label == True))\n",
    "    else:\n",
    "        label_identify = [i for i in np.unique(cubed_label) if i != 0][0]\n",
    "        coords = np.column_stack(np.where(cubed_label == label_identify))\n",
    "    if coords.shape[0] > 1000:\n",
    "        sampling_interval = coords.shape[0] // 1000\n",
    "    else:\n",
    "        sampling_interval = 1    \n",
    "\n",
    "    np.random.shuffle(coords)\n",
    "    subsampled = coords[::sampling_interval]\n",
    "    datamean = subsampled.mean(axis=0)\n",
    "    uu, dd, vv = np.linalg.svd(subsampled - datamean)\n",
    "    linepts = vv[0] * np.mgrid[-line_length:line_length:2j][:, np.newaxis]\n",
    "    linepts += datamean\n",
    "    return vv[0], linepts\n",
    "\n",
    "def view_saved_files(file_path: str) -> None:\n",
    "    ''' \n",
    "    Fxn for visualizing saved output files.\n",
    "    '''\n",
    "    dedicated_file_viewer = napari.Viewer()\n",
    "    contents = [c for c in os.listdir(file_path) if not c.startswith('.')]\n",
    "    for content in contents:\n",
    "        if content.endswith('.tif'):\n",
    "            if 'tub' in content or 'PI' in content:\n",
    "                dedicated_file_viewer.add_image(imread(os.path.join(file_path, content)), name=content.split('.')[0], blending='additive', visible=False)\n",
    "            else:\n",
    "                dedicated_file_viewer.add_labels(imread(os.path.join(file_path, content)), name=content.split('.')[0], blending='additive')\n",
    "        elif content.endswith('.txt'):\n",
    "            nums = np.loadtxt(os.path.join(file_path, content))\n",
    "            if nums.ndim == 1:\n",
    "                dedicated_file_viewer.add_points(nums, name=content.split('.')[0], face_color='white', blending='additive')\n",
    "            elif nums.ndim == 2:\n",
    "                dedicated_file_viewer.add_shapes(nums, shape_type='line', name=content.split('.')[0], edge_color='white', blending='additive')\n",
    "        else:\n",
    "            print(f'file \"{content}\" not imported to viewer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt \n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.17it/s]\n",
      "/Users/bementmbp/opt/anaconda3/envs/napari_apoc/lib/python3.9/site-packages/vispy/visuals/shaders/parsing.py:72: RuntimeWarning: coroutine 'get_msg' was never awaited\n",
      "  args = [tuple(arg.strip().split(' ')) for arg in args.split(',')]\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "  7%|▋         | 1/14 [01:31<19:52, 91.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E01\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:19<00:00,  2.05it/s]\n",
      " 14%|█▍        | 2/14 [04:03<25:23, 126.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E02\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:19<00:00,  3.10it/s]\n",
      " 21%|██▏       | 3/14 [07:29<29:53, 163.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E03\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:46<00:00,  1.97it/s]\n",
      " 29%|██▊       | 4/14 [13:46<41:15, 247.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E04\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.08it/s]\n",
      " 36%|███▌      | 5/14 [16:59<34:10, 227.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E05\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:23<00:00,  1.94it/s]\n",
      " 43%|████▎     | 6/14 [21:12<31:30, 236.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E06\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:38<00:00,  2.33it/s]\n",
      " 50%|█████     | 7/14 [29:50<38:20, 328.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Cntrl_E07\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:19<00:00,  2.84it/s]\n",
      " 57%|█████▋    | 8/14 [35:45<33:42, 337.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Exp_E01\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.54it/s]\n",
      " 64%|██████▍   | 9/14 [41:31<28:18, 339.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Exp_E02\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:29<00:00,  2.82it/s]\n",
      " 71%|███████▏  | 10/14 [52:22<29:02, 435.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Exp_E06\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:43<00:00,  1.86it/s]\n",
      " 79%|███████▊  | 11/14 [1:04:00<25:48, 516.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Exp_E08\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:42<00:00,  2.22it/s]\n",
      " 86%|████████▌ | 12/14 [1:19:12<21:13, 636.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Exp_E09\n",
      "Calculating point clouds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:32<00:00,  2.37it/s]\n",
      " 93%|█████████▎| 13/14 [1:32:55<07:08, 428.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with embryo Exp_E10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/bementmbp/Desktop/Scripts/volume-distribution/data_augmentation/make_spindle_cubes.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/data_augmentation/make_spindle_cubes.ipynb#ch0000001?line=13'>14</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(emb_save_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/data_augmentation/make_spindle_cubes.ipynb#ch0000001?line=15'>16</a>\u001b[0m \u001b[39m# define file paths and load data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/data_augmentation/make_spindle_cubes.ipynb#ch0000001?line=16'>17</a>\u001b[0m emb_type, emb_num \u001b[39m=\u001b[39m subdir\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/data_augmentation/make_spindle_cubes.ipynb#ch0000001?line=17'>18</a>\u001b[0m segmentations_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_\u001b[39m\u001b[39m{\u001b[39;00memb_type\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00memb_num\u001b[39m}\u001b[39;00m\u001b[39m-Z01_PI_16bit_scaleZ_sbdl2_16bit_seg.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bementmbp/Desktop/Scripts/volume-distribution/data_augmentation/make_spindle_cubes.ipynb#ch0000001?line=18'>19</a>\u001b[0m dog_tub_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_\u001b[39m\u001b[39m{\u001b[39;00memb_type\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00memb_num\u001b[39m}\u001b[39;00m\u001b[39m-Z01_Tub_16bit_scaleZ_sbdl2_16bit.tif\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "analysis_dir = '/Volumes/bigData/wholeMount_volDist/220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_Processed/N2V_Denoised/0_Analysis_01'\n",
    "subdirs = [d for d in os.listdir(analysis_dir) if os.path.isdir(os.path.join(analysis_dir, d))]\n",
    "\n",
    "# create save directory\n",
    "data_save_dir = os.path.join(analysis_dir, '0_data_cubes')\n",
    "if not os.path.exists(data_save_dir):\n",
    "    os.mkdir(data_save_dir)\n",
    "\n",
    "for subdir in tqdm(subdirs):\n",
    "\n",
    "    # make a save directory for this embryo\n",
    "    emb_save_dir = os.path.join(data_save_dir, subdir)\n",
    "    if not os.path.exists(emb_save_dir):\n",
    "        os.makedirs(emb_save_dir)\n",
    "\n",
    "    # define file paths and load data\n",
    "    emb_type, emb_num = subdir.split('_')\n",
    "    segmentations_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_{emb_num}-Z01_PI_16bit_scaleZ_sbdl2_16bit_seg.npy'\n",
    "    dog_tub_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_{emb_num}-Z01_Tub_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "    raw_tub_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_{emb_num}-Z01_Tub_16bit_scaleZ.tif'\n",
    "    pi_name = f'220712_Fix_Emb_Flvw_Chn1GAP_PI_aTub647_{emb_type}_{emb_num}-Z01_PI_16bit_scaleZ_sbdl2_16bit.tif'\n",
    "\n",
    "    data_load_dir = os.path.join(analysis_dir, subdir)\n",
    "    masks = np.load(os.path.join(data_load_dir, segmentations_name), allow_pickle=True).item()['masks']\n",
    "    tub = imread(os.path.join(data_load_dir, dog_tub_name))\n",
    "    tub_raw = imread(os.path.join(data_load_dir, raw_tub_name))\n",
    "    pi = imread(os.path.join(data_load_dir, pi_name))\n",
    "\n",
    "    # coarsly filter the masks of poor segmentations\n",
    "    minimum_size = 250000\n",
    "    maximum_size = 1000000\n",
    "    minimum_density = 0.21\n",
    "\n",
    "    filtered_masks = clear_border(masks)\n",
    "    filtered_masks = morphology.remove_small_objects(filtered_masks, min_size=minimum_size, connectivity=1)\n",
    "    filtered_masks = remove_large_objects(filtered_masks, max_size=maximum_size)\n",
    "    remaining_labels = [label for label in np.unique(filtered_masks) if label != 0]\n",
    "    print('Calculating point clouds...')\n",
    "    label_pcs = [return_points(masks, label_ID) for label_ID in tqdm(remaining_labels)]\n",
    "    densities = [find_label_density(pc) for pc in label_pcs]\n",
    "\n",
    "    for ind, id in enumerate(remaining_labels):\n",
    "        if id in remaining_labels and densities[ind] < minimum_density:\n",
    "            filtered_masks[filtered_masks == id] = 0\n",
    "\n",
    "    # Establish the remaining labels for the embryo and loop through each cell\n",
    "    final_labels = [label for label in np.unique(filtered_masks) if label != 0]\n",
    "    for curr_mask_id in final_labels:\n",
    "        wipe_layers(viewer)\n",
    "\n",
    "        # isolate the current mask as bolean array\n",
    "        curr_mask = filtered_masks == curr_mask_id\n",
    "\n",
    "        # get the coordinate of the bounding cube for the current mask ID. Apply it to the labels and images\n",
    "        cube_dims = get_cube(filtered_masks, curr_mask_id)\n",
    "        cubed_label = apply_cube(curr_mask, cube_dims)\n",
    "        cubed_tub = apply_cube(tub, cube_dims)\n",
    "        cubed_tub_raw = apply_cube(tub_raw, cube_dims)\n",
    "        cubed_PI = apply_cube(pi, cube_dims)\n",
    "\n",
    "        # get the mask coordinates, centroid, and long axis\n",
    "        mask_coords = np.column_stack(np.where(cubed_label == True))\n",
    "        cell_centroid = mask_coords.mean(axis=0)\n",
    "        cell_long_vect, cell_long_line = get_long_axis(cubed_label)\n",
    "\n",
    "        # erode the mask to eliminate some cortical signal\n",
    "        eroded_mask = binary_erosion(cubed_label, footprint=np.ones((3, 3, 3)))\n",
    "        for i in range(10):\n",
    "            eroded_mask = binary_erosion(eroded_mask)\n",
    "\n",
    "        # get the tubulin signal from the remaining region and define an Otsu threshold\n",
    "        remaining_tub = np.zeros(shape=cubed_label.shape)\n",
    "        remaining_tub[eroded_mask] = cubed_tub[eroded_mask]\n",
    "        remaining_vals = cubed_tub[eroded_mask].ravel()\n",
    "        thresh_val = threshold_otsu(remaining_vals)\n",
    "        thresh_mask = label(remaining_tub > thresh_val)\n",
    "\n",
    "        # get number of remaining labels\n",
    "        num_tub_labels_b4_filter = [label for label in np.unique(thresh_mask) if label != 0]\n",
    "\n",
    "        if len(num_tub_labels_b4_filter) == 0:\n",
    "            # no filtering required if we already have no labels\n",
    "            print(f'no labels detected in {curr_mask_id}')\n",
    "            continue\n",
    "\n",
    "        # filter and labels smaller than the mimum and maximum expected label sizes\n",
    "        min_thrsh_size = 100   \n",
    "        if len(num_tub_labels_b4_filter) > 1:\n",
    "            thresh_mask = morphology.remove_small_objects(thresh_mask, min_size=100, connectivity=1)\n",
    "\n",
    "        # get the number of labels after filtering\n",
    "        remaining_labels = [label for label in np.unique(thresh_mask) if label != 0]\n",
    "\n",
    "        if len(remaining_labels) == 0:\n",
    "            print(f'only tiny objects detected in mask {curr_mask_id}')\n",
    "            continue\n",
    "\n",
    "        # define mask save directory\n",
    "        mask_save_dir = os.path.join(emb_save_dir, f'cell_{curr_mask_id}')\n",
    "        if not os.path.exists(mask_save_dir):\n",
    "            os.mkdir(mask_save_dir)\n",
    "\n",
    "        # populate the viewer \n",
    "        viewer.add_labels(eroded_mask, name='eroded_mask', blending='additive', visible=False)\n",
    "        viewer.add_labels(cubed_label, name='curr_mask_cube', blending='additive')\n",
    "        viewer.add_image(cubed_tub, name='curr_tub_cube', blending='additive', visible=False)\n",
    "        viewer.add_image(cubed_tub_raw, name='curr_tub_raw_cube', blending='additive', visible=False)\n",
    "        viewer.add_image(cubed_PI, name='curr_PI_cube', blending='additive', visible=False)\n",
    "        viewer.add_labels(thresh_mask, name='thresh_mask', blending='additive')\n",
    "\n",
    "        images_and_layers = ['curr_mask_cube',\n",
    "                            'curr_tub_cube',\n",
    "                            'curr_tub_raw_cube',\n",
    "                            'curr_PI_cube',\n",
    "                            'eroded_mask',\n",
    "                            'thresh_mask']\n",
    "\n",
    "        # save the tif compatible layers as tifs\n",
    "        for item in images_and_layers:\n",
    "            viewer.layers[item].save(os.path.join(mask_save_dir, item + '.tif'))\n",
    "        \n",
    "    print(f'finished with embryo {subdir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('napari_apoc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9332436963653b5df92bf2f53404d912d66b2e5a7a0f130fd2cb37eb34a8db49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
